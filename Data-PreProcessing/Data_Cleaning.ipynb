{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Loading Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc  # garbage collector\n",
    "\n",
    "from fastcore.basics import *\n",
    "from fastcore.parallel import *\n",
    "from numba import jit, njit, vectorize, cuda, uint32, f8, uint8\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics  # for accuracy calculation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from functools import partial\n",
    "from os import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from pylab import imshow, show\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up paths to csv files / datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CSV-01-12\n",
    "path_DdoS_DNS = \"C:\\\\CIC-DDOS-2019\\\\DrDoS_DNS.csv\"\n",
    "path_DdoS_MSSQL = \"C:\\\\CIC-DDOS-2019\\\\DrDoS_MSSQL.csv\"\n",
    "path_DdoS_LDAP = \"C:\\\\CIC-DDOS-2019\\\\DrDoS_LDAP.csv\"\n",
    "path_DdoS_NTP = \"C:\\\\CIC-DDOS-2019\\\\DrDoS_NTP.csv\"\n",
    "path_DdoS_NetBIOS = \"C:\\\\CIC-DDOS-2019\\\\DrDoS_NetBIOS.csv\"\n",
    "path_DdoS_SNMP = \"C:\\\\CIC-DDOS-2019\\\\DrDoS_SNMP.csv\"\n",
    "path_DdoS_SSDP = \"C:\\\\CIC-DDOS-2019\\\\DrDoS_SSDP.csv\"\n",
    "path_DdoS_UDP = \"C:\\\\CIC-DDOS-2019\\\\DrDoS_UDP.csv\"\n",
    "path_Syn = \"C:\\\\CIC-DDOS-2019\\\\Syn.csv\"\n",
    "path_TFTP = \"C:\\\\CIC-DDOS-2019\\\\TFTP.csv\"\n",
    "path_UDPLag = \"C:\\\\CIC-DDOS-2019\\\\UDPLag.csv\"\n",
    "\n",
    "# # CSV-03-11\n",
    "# path__LDAP = \"../CICDDoS-2019/CSV-03-11/03-11/LDAP.csv\"\n",
    "# path__MSSQL = \"../CICDDoS-2019/CSV-03-11/03-11/MSSQL.csv\"\n",
    "# path__NetBIOS = \"../CICDDoS-2019/CSV-03-11/03-11/NetBIOS.csv\"\n",
    "# path__Portmap = \"../CICDDoS-2019/CSV-03-11/03-11/Portmap.csv\"\n",
    "# path__Syn = \"../CICDDoS-2019/CSV-03-11/03-11/Syn.csv\"\n",
    "# path__UDP = \"../CICDDoS-2019/CSV-03-11/03-11/UDP.csv\"\n",
    "# path__UDPLag = \"../CICDDoS-2019/CSV-03-11/03-11/UDPLag.csv\"\n",
    "\n",
    "paths = [path_DdoS_DNS, path_DdoS_MSSQL, path_DdoS_LDAP, path_DdoS_NTP, path_DdoS_NetBIOS, path_DdoS_SNMP,\n",
    "         path_DdoS_SSDP, path_DdoS_UDP, path_Syn, path_TFTP, path_UDPLag]\n",
    "# , path__LDAP, path__MSSQL, path__NetBIOS,\n",
    "#      path__Portmap, path__Syn, path__UDP, path__UDPLag]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column / feature names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "col_name_consistency = {\n",
    "    'Flow ID': 'Flow ID',\n",
    "    'Source IP': 'Source IP',\n",
    "    'Src IP': 'Source IP',\n",
    "    'Source Port': 'Source Port',\n",
    "    'Src Port': 'Source Port',\n",
    "    'Destination IP': 'Destination IP',\n",
    "    'Dst IP': 'Destination IP',\n",
    "    'Destination Port': 'Destination Port',\n",
    "    'Dst Port': 'Destination Port',\n",
    "    'Protocol': 'Protocol',\n",
    "    'Timestamp': 'Timestamp',\n",
    "    'Flow Duration': 'Flow Duration',\n",
    "    'Total Fwd Packets': 'Total Fwd Packets',\n",
    "    'Tot Fwd Pkts': 'Total Fwd Packets',\n",
    "    'Total Backward Packets': 'Total Backward Packets',\n",
    "    'Tot Bwd Pkts': 'Total Backward Packets',\n",
    "    'Total Length of Fwd Packets': 'Fwd Packets Length Total',\n",
    "    'TotLen Fwd Pkts': 'Fwd Packets Length Total',\n",
    "    'Total Length of Bwd Packets': 'Bwd Packets Length Total',\n",
    "    'TotLen Bwd Pkts': 'Bwd Packets Length Total',\n",
    "    'Fwd Packet Length Max': 'Fwd Packet Length Max',\n",
    "    'Fwd Pkt Len Max': 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min': 'Fwd Packet Length Min',\n",
    "    'Fwd Pkt Len Min': 'Fwd Packet Length Min',\n",
    "    'Fwd Packet Length Mean': 'Fwd Packet Length Mean',\n",
    "    'Fwd Pkt Len Mean': 'Fwd Packet Length Mean',\n",
    "    'Fwd Packet Length Std': 'Fwd Packet Length Std',\n",
    "    'Fwd Pkt Len Std': 'Fwd Packet Length Std',\n",
    "    'Bwd Packet Length Max': 'Bwd Packet Length Max',\n",
    "    'Bwd Pkt Len Max': 'Bwd Packet Length Max',\n",
    "    'Bwd Packet Length Min': 'Bwd Packet Length Min',\n",
    "    'Bwd Pkt Len Min': 'Bwd Packet Length Min',\n",
    "    'Bwd Packet Length Mean': 'Bwd Packet Length Mean',\n",
    "    'Bwd Pkt Len Mean': 'Bwd Packet Length Mean',\n",
    "    'Bwd Packet Length Std': 'Bwd Packet Length Std',\n",
    "    'Bwd Pkt Len Std': 'Bwd Packet Length Std',\n",
    "    'Flow Bytes/s': 'Flow Bytes/s',\n",
    "    'Flow Byts/s': 'Flow Bytes/s',\n",
    "    'Flow Packets/s': 'Flow Packets/s',\n",
    "    'Flow Pkts/s': 'Flow Packets/s',\n",
    "    'Flow IAT Mean': 'Flow IAT Mean',\n",
    "    'Flow IAT Std': 'Flow IAT Std',\n",
    "    'Flow IAT Max': 'Flow IAT Max',\n",
    "    'Flow IAT Min': 'Flow IAT Min',\n",
    "    'Fwd IAT Total': 'Fwd IAT Total',\n",
    "    'Fwd IAT Tot': 'Fwd IAT Total',\n",
    "    'Fwd IAT Mean': 'Fwd IAT Mean',\n",
    "    'Fwd IAT Std': 'Fwd IAT Std',\n",
    "    'Fwd IAT Max': 'Fwd IAT Max',\n",
    "    'Fwd IAT Min': 'Fwd IAT Min',\n",
    "    'Bwd IAT Total': 'Bwd IAT Total',\n",
    "    'Bwd IAT Tot': 'Bwd IAT Total',\n",
    "    'Bwd IAT Mean': 'Bwd IAT Mean',\n",
    "    'Bwd IAT Std': 'Bwd IAT Std',\n",
    "    'Bwd IAT Max': 'Bwd IAT Max',\n",
    "    'Bwd IAT Min': 'Bwd IAT Min',\n",
    "    'Fwd PSH Flags': 'Fwd PSH Flags',\n",
    "    'Bwd PSH Flags': 'Bwd PSH Flags',\n",
    "    'Fwd URG Flags': 'Fwd URG Flags',\n",
    "    'Bwd URG Flags': 'Bwd URG Flags',\n",
    "    'Fwd Header Length': 'Fwd Header Length',\n",
    "    'Fwd Header Len': 'Fwd Header Length',\n",
    "    'Bwd Header Length': 'Bwd Header Length',\n",
    "    'Bwd Header Len': 'Bwd Header Length',\n",
    "    'Fwd Packets/s': 'Fwd Packets/s',\n",
    "    'Fwd Pkts/s': 'Fwd Packets/s',\n",
    "    'Bwd Packets/s': 'Bwd Packets/s',\n",
    "    'Bwd Pkts/s': 'Bwd Packets/s',\n",
    "    'Min Packet Length': 'Packet Length Min',\n",
    "    'Pkt Len Min': 'Packet Length Min',\n",
    "    'Max Packet Length': 'Packet Length Max',\n",
    "    'Pkt Len Max': 'Packet Length Max',\n",
    "    'Packet Length Mean': 'Packet Length Mean',\n",
    "    'Pkt Len Mean': 'Packet Length Mean',\n",
    "    'Packet Length Std': 'Packet Length Std',\n",
    "    'Pkt Len Std': 'Packet Length Std',\n",
    "    'Packet Length Variance': 'Packet Length Variance',\n",
    "    'Pkt Len Var': 'Packet Length Variance',\n",
    "    'FIN Flag Count': 'FIN Flag Count',\n",
    "    'FIN Flag Cnt': 'FIN Flag Count',\n",
    "    'SYN Flag Count': 'SYN Flag Count',\n",
    "    'SYN Flag Cnt': 'SYN Flag Count',\n",
    "    'RST Flag Count': 'RST Flag Count',\n",
    "    'RST Flag Cnt': 'RST Flag Count',\n",
    "    'PSH Flag Count': 'PSH Flag Count',\n",
    "    'PSH Flag Cnt': 'PSH Flag Count',\n",
    "    'ACK Flag Count': 'ACK Flag Count',\n",
    "    'ACK Flag Cnt': 'ACK Flag Count',\n",
    "    'URG Flag Count': 'URG Flag Count',\n",
    "    'URG Flag Cnt': 'URG Flag Count',\n",
    "    'CWE Flag Count': 'CWE Flag Count',\n",
    "    'CWE Flag Cnt': 'CWE Flag Count',\n",
    "    'ECE Flag Count': 'ECE Flag Count',\n",
    "    'ECE Flag Cnt': 'ECE Flag Count',\n",
    "    'Down/Up Ratio': 'Down/Up Ratio',\n",
    "    'Average Packet Size': 'Avg Packet Size',\n",
    "    'Pkt Size Avg': 'Avg Packet Size',\n",
    "    'Avg Fwd Segment Size': 'Avg Fwd Segment Size',\n",
    "    'Fwd Seg Size Avg': 'Avg Fwd Segment Size',\n",
    "    'Avg Bwd Segment Size': 'Avg Bwd Segment Size',\n",
    "    'Bwd Seg Size Avg': 'Avg Bwd Segment Size',\n",
    "    'Fwd Avg Bytes/Bulk': 'Fwd Avg Bytes/Bulk',\n",
    "    'Fwd Byts/b Avg': 'Fwd Avg Bytes/Bulk',\n",
    "    'Fwd Avg Packets/Bulk': 'Fwd Avg Packets/Bulk',\n",
    "    'Fwd Pkts/b Avg': 'Fwd Avg Packets/Bulk',\n",
    "    'Fwd Avg Bulk Rate': 'Fwd Avg Bulk Rate',\n",
    "    'Fwd Blk Rate Avg': 'Fwd Avg Bulk Rate',\n",
    "    'Bwd Avg Bytes/Bulk': 'Bwd Avg Bytes/Bulk',\n",
    "    'Bwd Byts/b Avg': 'Bwd Avg Bytes/Bulk',\n",
    "    'Bwd Avg Packets/Bulk': 'Bwd Avg Packets/Bulk',\n",
    "    'Bwd Pkts/b Avg': 'Bwd Avg Packets/Bulk',\n",
    "    'Bwd Avg Bulk Rate': 'Bwd Avg Bulk Rate',\n",
    "    'Bwd Blk Rate Avg': 'Bwd Avg Bulk Rate',\n",
    "    'Subflow Fwd Packets': 'Subflow Fwd Packets',\n",
    "    'Subflow Fwd Pkts': 'Subflow Fwd Packets',\n",
    "    'Subflow Fwd Bytes': 'Subflow Fwd Bytes',\n",
    "    'Subflow Fwd Byts': 'Subflow Fwd Bytes',\n",
    "    'Subflow Bwd Packets': 'Subflow Bwd Packets',\n",
    "    'Subflow Bwd Pkts': 'Subflow Bwd Packets',\n",
    "    'Subflow Bwd Bytes': 'Subflow Bwd Bytes',\n",
    "    'Subflow Bwd Byts': 'Subflow Bwd Bytes',\n",
    "    'Init_Win_bytes_forward': 'Init Fwd Win Bytes',\n",
    "    'Init Fwd Win Byts': 'Init Fwd Win Bytes',\n",
    "    'Init_Win_bytes_backward': 'Init Bwd Win Bytes',\n",
    "    'Init Bwd Win Byts': 'Init Bwd Win Bytes',\n",
    "    'act_data_pkt_fwd': 'Fwd Act Data Packets',\n",
    "    'Fwd Act Data Pkts': 'Fwd Act Data Packets',\n",
    "    'min_seg_size_forward': 'Fwd Seg Size Min',\n",
    "    'Fwd Seg Size Min': 'Fwd Seg Size Min',\n",
    "    'Active Mean': 'Active Mean',\n",
    "    'Active Std': 'Active Std',\n",
    "    'Active Max': 'Active Max',\n",
    "    'Active Min': 'Active Min',\n",
    "    'Idle Mean': 'Idle Mean',\n",
    "    'Idle Std': 'Idle Std',\n",
    "    'Idle Max': 'Idle Max',\n",
    "    'Idle Min': 'Idle Min',\n",
    "    'Label': 'Label'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following Columns may have little insignificance over model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drop_columns = [  # this list includes all spellings across CIC NIDS datasets\n",
    "    \"Flow ID\",\n",
    "    'Fwd Header Length.1',\n",
    "    \"Source IP\",\n",
    "    \"Src IP\",\n",
    "    \"Source Port\",\n",
    "    \"Src Port\",\n",
    "    \"Destination IP\",\n",
    "    \"Dst IP\",\n",
    "    \"Destination Port\",\n",
    "    \"Dst Port\",\n",
    "    \"Timestamp\",\n",
    "    \"Unnamed: 0\",\n",
    "    \"Inbound\",\n",
    "    \"SimillarHTTP\"  # CIC-DDoS other undocumented columns\n",
    "]\n",
    "len(drop_columns)  # src_port,dst_port, src_ip, dst_ip these are duplicate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# print(device_lib.list_local_devices())\n",
    "print(\"Num of GPUs available: \", len(tf.test.gpu_device_name()))\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "# import cudf\n",
    "# dtypes = {'SimillarHTTP': 'object'}\n",
    "# df_gpu = cudf.read_csv(path_TFTP, blocksize=50e6, low_memory=False, dtype=dtypes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def readHugeCsvFileAsDataFrame(file_path):\n",
    "    # start = timer()\n",
    "    dtypes = {'SimillarHTTP': 'object',\n",
    "              'Timestamp': 'object',\n",
    "              'Source IP': 'str',\n",
    "              'Destination IP': 'str',\n",
    "              'Flow ID': 'object',\n",
    "              'Label': 'object',\n",
    "              }\n",
    "    for feature in [f'f_{i}' for i in range(82)]:\n",
    "        dtypes[feature] = \"float32\"\n",
    "\n",
    "    dask_df = dd.read_csv(file_path, low_memory=False,blocksize=50000, dtype=dtypes)  # 50MB chunk-size\n",
    "    # elapsed_time = timer() - start\n",
    "    # print(\"Read csv with dask: \", elapsed_time, \"sec\")\n",
    "    return dask_df.compute()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scheduler = Client()\n",
    "scheduler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Without Scheduler (see the elapsed time to process)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "%%time\n",
    "# workingDataFrame = readHugeCsvFileAsDataFrame(path_UDPLag)\n",
    "# workingDataFrame = readHugeCsvFileAsDataFrame(path_Syn)\n",
    "# workingDataFrame = readHugeCsvFileAsDataFrame(path_DdoS_NTP)\n",
    "# workingDataFrame = readHugeCsvFileAsDataFrame(path_DdoS_LDAP)\n",
    "# workingDataFrame = readHugeCsvFileAsDataFrame(path_DdoS_SSDP)\n",
    "workingDataFrame = readHugeCsvFileAsDataFrame(path_DdoS_UDP)\n",
    "# workingDataFrame.describe()\n",
    "workingDataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With Scheduler (see the elapsed time to process)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "df = scheduler.submit(readHugeCsvFileAsDataFrame, path_DdoS_UDP)\n",
    "workingDataFrame = df.result()\n",
    "workingDataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dropping Unnecessary Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.columns = workingDataFrame.columns.str.strip()  # sometimes there's leading / trailing whitespace\n",
    "workingDataFrame.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "workingDataFrame.rename(columns=col_name_consistency, inplace=True)\n",
    "\n",
    "workingDataFrame.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# workingDataFrame.dtypes\n",
    "workingDataFrame.info(memory_usage=\"deep\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Cleaning Based on Data Types (DownSizing)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for column in workingDataFrame:\n",
    "    if workingDataFrame[column].dtype == 'float64':\n",
    "        workingDataFrame[column] = pd.to_numeric(workingDataFrame[column], downcast='float')\n",
    "    if workingDataFrame[column].dtype == 'int64':\n",
    "        workingDataFrame[column] = pd.to_numeric(workingDataFrame[column], downcast='integer')\n",
    "\n",
    "workingDataFrame.info(memory_usage=\"deep\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing NaN values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.isna().any(axis=1).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "workingDataFrame.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After Dropping NaN values, want to see total rows.\n",
    "That's why, Converting pandas dataFrame into dask's dataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(workingDataFrame, npartitions=10)\n",
    "ddf.compute()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dropping Duplicates\n",
    "There should be no duplicates because they can bias training and can lead to over-optimistic estimates of classification performance during testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.duplicated().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fully duplicate rows to be removed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.drop_duplicates(inplace=True)\n",
    "workingDataFrame.reset_index(inplace=True, drop=True)\n",
    "workingDataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After data cleaning, how much storage it holds now"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.info(memory_usage=\"deep\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columnList = workingDataFrame.columns.tolist()\n",
    "# columnList[:-1]  # just for now, Omitting 'label' column\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_thr = VarianceThreshold(threshold=0.1)  #Removing both constant and quasi-constant\n",
    "var_thr.fit(workingDataFrame[columnList[:-1]])\n",
    "\n",
    "var_threshold_bool_list = var_thr.get_support()\n",
    "var_threshold_bool_list_after_label_added = np.append(var_threshold_bool_list,True)\n",
    "var_threshold_bool_list_after_label_added"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "concol = [column for column in workingDataFrame.columns\n",
    "          if column not in workingDataFrame.columns[var_threshold_bool_list_after_label_added]]\n",
    "\n",
    "for omittedFeatures in concol:\n",
    "    print(omittedFeatures)\n",
    "\n",
    "workingDataFrame.drop(columns=concol, inplace=True, errors='ignore')\n",
    "workingDataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now arising problem is that, row number has been significantly reduced but feature numbers are still 78!!\n",
    "So, need feature engineering here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def featueEngineeringBasedOnZero(dataFrameArg, thresholdPercentage, showPercentage):\n",
    "    totalCols = dataFrameArg.shape[1]\n",
    "    totalRows = len(dataFrameArg)\n",
    "    unNecessaryFeatureCount = 0\n",
    "    unNecessaryFeatureNames = []\n",
    "\n",
    "    for column in dataFrameArg:\n",
    "        zerosInCol = (dataFrameArg[column] == 0).sum()\n",
    "        if zerosInCol != 0:\n",
    "            percentageOfZerosInRow = ((zerosInCol * 100) / totalRows)\n",
    "\n",
    "            if showPercentage:\n",
    "                print(column, \"\\t\\t-\\t\\t\", zerosInCol, \"\\t\\t-\\t\\t\", percentageOfZerosInRow)\n",
    "\n",
    "            if percentageOfZerosInRow > thresholdPercentage:\n",
    "                unNecessaryFeatureNames.append(column)\n",
    "                unNecessaryFeatureCount = unNecessaryFeatureCount + 1\n",
    "\n",
    "    print(\"\\nTotal features having more than \", thresholdPercentage, \"% zero are - \", unNecessaryFeatureCount,\n",
    "          \"out of \",\n",
    "          totalCols)\n",
    "    return unNecessaryFeatureNames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Identifying those features containing 99% zeroes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "featureContainingAlmostZero = featueEngineeringBasedOnZero(dataFrameArg=workingDataFrame, thresholdPercentage=99,\n",
    "                                                           showPercentage=False)\n",
    "featureContainingAlmostZero"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Omitting above features containing 99% zeroes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.drop(columns=featureContainingAlmostZero, inplace=True, errors='ignore')\n",
    "workingDataFrame.rename(columns=col_name_consistency, inplace=True)\n",
    "workingDataFrame.reset_index(inplace=True, drop=True)\n",
    "workingDataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, To see the number of unique values in each column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame.nunique(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "workingDataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving New DataFrame as csv file to new location"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dataCleaningResultToAnotherCSV(dataFrameArg, dirPath, file_name):\n",
    "    dataFrameArg.to_csv(dirPath + file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newCsvPath = \"C:\\\\CIC-DDOS-2019\\\\AfterDataCleaning\\\\\"\n",
    "# newFileName = \"UDPLag.csv\"\n",
    "# newFileName = \"Syn.csv\"\n",
    "# newFileName = \"DrDoS_NTP.csv\"\n",
    "# newFileName = \"DrDoS_LDAP.csv\"\n",
    "# newFileName = \"DrDoS_SSDP.csv\"\n",
    "newFileName = \"DrDoS_UDP.csv\"\n",
    "dataCleaningResultToAnotherCSV(dataFrameArg=workingDataFrame, dirPath=newCsvPath, file_name=newFileName)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
